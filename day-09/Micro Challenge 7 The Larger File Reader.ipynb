{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2470ffe3-8743-485c-8c2d-f4d7cceba68c",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "- Write a **generator function**\n",
    "- That reads a **very large file** (for example, a 100GB file)\n",
    "- Reads the file **line by line**\n",
    "\n",
    "The entire file should **NOT** be loaded into memory at once.\n",
    "\n",
    "---\n",
    "\n",
    "## Deep Dive (Why This Is Important)\n",
    "\n",
    "### What Is the Problem?\n",
    "\n",
    "Suppose you write:\n",
    "\n",
    "```python\n",
    "lines = open(\"bigfile.txt\").readlines()\n",
    "```\n",
    "### This tries to load the entire file into RAM\n",
    "\n",
    "If the file is very large (e.g., 100GB) and your RAM is smaller\n",
    "The program will crash or freeze\n",
    "This is why reading large files all at once is unsafe.\n",
    "\n",
    "\n",
    "## Problem with Loading the Whole File\n",
    "\n",
    " Issues:\n",
    "\n",
    "- The entire file is loaded into RAM\n",
    "- If the file is 100GB and the system has only 16GB RAM  \n",
    "  â†’ the program will crash\n",
    "\n",
    "---\n",
    "\n",
    "## Why a Generator Is Needed Here\n",
    "\n",
    "When using a generator with `yield`:\n",
    "\n",
    "- One line is read from the file\n",
    "- The line is processed immediately\n",
    "- The line is then released from memory\n",
    "- The next line is read\n",
    "\n",
    "All data is **never stored in memory at the same time**.\n",
    "\n",
    "This approach is called **Lazy Evaluation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3783e-493e-481d-a03a-95d82e58adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_large_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af67a65-82fe-441b-8949-cfb404b80fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in read_large_file(\"bigfile.txt\"):\n",
    "    process(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281095f-1abd-47d0-9d28-58b020c388d8",
   "metadata": {},
   "source": [
    "## Key Characteristics\n",
    "\n",
    "- One line is processed at a time  \n",
    "- RAM usage stays almost constant  \n",
    "- The program remains safe, no matter how large the file is  \n",
    "\n",
    "---\n",
    "\n",
    "## Why This Is the Standard in Big Data Processing\n",
    "\n",
    "This approach is used because:\n",
    "\n",
    "- Big data is often **larger than available RAM**\n",
    "- Generators process data as a **stream**, not all at once\n",
    "- The same technique is commonly used in:\n",
    "  - Log processing\n",
    "  - CSV / JSONL parsing\n",
    "  - ETL pipelines\n",
    "  - Machine Learning data loading\n",
    "\n",
    "---\n",
    "\n",
    "## One-Line Rule to Remember\n",
    "\n",
    "> **If data is bigger than RAM, use generators.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fb673-6cfc-4e88-a62b-ac13bd10d25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
